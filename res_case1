/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-13 02:05:41,231] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2024-11-13 02:05:43,239] [INFO] [runner.py:463:main] Using IP address of 10.242.51.166 for node JF5300-B11A319T
nkbhat_dbg: IMPI calling get_cmd from multinode_runner
['mpirun', '-ppn', '2', '-genv', 'PYTHONPATH', '/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference', '-genv', 'OMP_NUM_THREADS', '85', '-genv', 'MASTER_ADDR', '10.242.51.166', '-genv', 'MASTER_PORT', '29500', '-genv', 'WORLD_SIZE', '2', '-genv', 'LOCAL_SIZE', '2', '-genv', 'I_MPI_PIN', '0', '-hosts', 'JF5300-B11A319T', '-n', '1', '-env', 'RANK', '0', '-env', 'LOCAL_RANK', '0', 'numactl', '-C', '0-84', '/root/miniforge3/envs/llm/bin/python', '-u', 'run.py', '--benchmark', '-m', './saved_results/llama_8b_shard', '--dtype', 'bfloat16', '--ipex', '--greedy', '--input-tokens', '1024', '--num-iter', '2', '--num-warmup', '1', '--batch-size', '1', '--max-new-tokens', '32', '--token-latency', '--autotp', ':', '-n', '1', '-env', 'RANK', '1', '-env', 'LOCAL_RANK', '1', 'numactl', '-C', '85,171-254', '/root/miniforge3/envs/llm/bin/python', '-u', 'run.py', '--benchmark', '-m', './saved_results/llama_8b_shard', '--dtype', 'bfloat16', '--ipex', '--greedy', '--input-tokens', '1024', '--num-iter', '2', '--num-warmup', '1', '--batch-size', '1', '--max-new-tokens', '32', '--token-latency', '--autotp']
[2024-11-13 02:05:43,286] [INFO] [runner.py:568:main] cmd = mpirun -ppn 2 -genv PYTHONPATH /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference -genv OMP_NUM_THREADS 85 -genv MASTER_ADDR 10.242.51.166 -genv MASTER_PORT 29500 -genv WORLD_SIZE 2 -genv LOCAL_SIZE 2 -genv I_MPI_PIN 0 -hosts JF5300-B11A319T -n 1 -env RANK 0 -env LOCAL_RANK 0 numactl -C 0-84 /root/miniforge3/envs/llm/bin/python -u run.py --benchmark -m ./saved_results/llama_8b_shard --dtype bfloat16 --ipex --greedy --input-tokens 1024 --num-iter 2 --num-warmup 1 --batch-size 1 --max-new-tokens 32 --token-latency --autotp : -n 1 -env RANK 1 -env LOCAL_RANK 1 numactl -C 85,171-254 /root/miniforge3/envs/llm/bin/python -u run.py --benchmark -m ./saved_results/llama_8b_shard --dtype bfloat16 --ipex --greedy --input-tokens 1024 --num-iter 2 --num-warmup 1 --batch-size 1 --max-new-tokens 32 --token-latency --autotp
LLM RUNTIME INFO: running model geneartion with deepspeed (autotp)...
LLM RUNTIME INFO: running model geneartion with deepspeed (autotp)...
/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-13 02:05:45,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-13 02:05:45,994] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Error.  nthreads cannot be larger than environment variable "NUMEXPR_MAX_THREADS" (64)Error.  nthreads cannot be larger than environment variable "NUMEXPR_MAX_THREADS" (64)[2024-11-13 02:05:47,182] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-13 02:05:47,182] [INFO] [comm.py:637:init_distributed] cdb=None
Using /root/.cache/torch_extensions/py310_cpu as PyTorch extensions root...
[2024-11-13 02:05:47,211] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-13 02:05:47,211] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-13 02:05:47,212] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl
Using /root/.cache/torch_extensions/py310_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cpu/deepspeed_shm_comm/build.ninja...
Building extension module deepspeed_shm_comm...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module deepspeed_shm_comm...
Loading extension module deepspeed_shm_comm...
Time to load deepspeed_shm_comm op: 0.10066866874694824 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
*** Loading the model /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/saved_results/llama_8b_shard
[2024-11-13 02:05:48,593] [INFO] [utils.py:781:see_memory_usage] pre-from-pretrained
[2024-11-13 02:05:48,593] [INFO] [utils.py:782:see_memory_usage] MA 0.63 GB         Max_MA 0.63 GB         CA 0.63 GB         Max_CA 1 GB 
[2024-11-13 02:05:48,594] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.72 GB, percent = 1.9%
[2024-11-13 02:05:48,747] [INFO] [utils.py:781:see_memory_usage] post-from-pretrained
[2024-11-13 02:05:48,748] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-13 02:05:48,748] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.72 GB, percent = 1.9%
[2024-11-13 02:05:48,864] [INFO] [utils.py:781:see_memory_usage] post-init-ds-zero-init
[2024-11-13 02:05:48,864] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-13 02:05:48,864] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.72 GB, percent = 1.9%
[2024-11-13 02:05:48,977] [INFO] [utils.py:781:see_memory_usage] pre-ds-inference-init
[2024-11-13 02:05:48,977] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-13 02:05:48,977] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.72 GB, percent = 1.9%
My guessed rank = 0
[cli_0]: write_line error; fd=5 buf=:cmd=init pmi_version=1 pmi_subversion=1
:
system msg for write_line failure : Invalid argument
[cli_0]: Unable to write to PMI_fd
[cli_0]: write_line error; fd=5 buf=:cmd=get_appnum
:
system msg for write_line failure : Invalid argument
Abort(1090319) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init_thread: Unknown error class, error stack:
MPIR_Init_thread(192): 
MPID_Init(1538)......: 
MPIR_pmi_init(131)...: PMI_Get_appnum returned -1
[cli_0]: write_line error; fd=5 buf=:cmd=abort exitcode=1090319
:
system msg for write_line failure : Invalid argument
My guessed rank = 1
[cli_1]: Error reading initack on 6
Error on readline:: Bad file descriptor
[cli_1]: readline failed
[cli_1]: readline failed
[cli_1]: readline failed
[cli_1]: readline failed
Abort(1090319) on node 0 (rank 0 in comm 0): Fatal error in PMPI_Init_thread: Unknown error class, error stack:
MPIR_Init_thread(192): 
MPID_Init(1538)......: 
MPIR_pmi_init(131)...: PMI_Get_appnum returned -1
[cli_1]: readline failed
Traceback (most recent call last):
  File "/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/_inductor/compile_worker/__main__.py", line 45, in <module>
    main()
  File "/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/_inductor/compile_worker/__main__.py", line 41, in main
    SubprocMain(args.workers, read_fd, write_fd).main()
  File "/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 180, in main
    job_id, data = _recv_msg(self.read_pipe)
  File "/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 58, in _recv_msg
    data = read_pipe.read(length) if length > 0 else b""
MemoryError
LLM RUNTIME ERROR: Running generation task failed. Quit.
LLM RUNTIME ERROR: Running generation task failed. Quit.

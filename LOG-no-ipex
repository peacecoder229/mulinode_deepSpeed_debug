/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 22:28:47,383] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2024-11-03 22:28:49,152] [INFO] [runner.py:463:main] Using IP address of 10.242.51.166 for node 10.242.51.166
['mpirun', '-ppn', '1', '-genv', 'PYTHONPATH', '/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference', '-genv', 'OMP_NUM_THREADS', '128', '-genv', 'MASTER_ADDR', '10.242.51.166', '-genv', 'MASTER_PORT', '29500', '-genv', 'WORLD_SIZE', '2', '-genv', 'LOCAL_SIZE', '1', '-genv', 'I_MPI_PIN', '0', '-hosts', '10.242.51.166,10.242.51.116', '-n', '1', '-env', 'RANK', '0', '-env', 'LOCAL_RANK', '0', 'numactl', '-C', '0-127', '/root/miniforge3/envs/llm/bin/python', '-u', 'run.py', '--benchmark', '-m', './saved_results/llama_8b_shard/', '--dtype', 'bfloat16', '--greedy', '--input-tokens', '1024', '--num-iter', '1', '--batch-size', '1', '--max-new-tokens', '56', '--token-latency', '--autotp', ':', '-n', '1', '-env', 'RANK', '1', '-env', 'LOCAL_RANK', '0', 'numactl', '-C', '0-127', '/root/miniforge3/envs/llm/bin/python', '-u', 'run.py', '--benchmark', '-m', './saved_results/llama_8b_shard/', '--dtype', 'bfloat16', '--greedy', '--input-tokens', '1024', '--num-iter', '1', '--batch-size', '1', '--max-new-tokens', '56', '--token-latency', '--autotp']
[2024-11-03 22:28:49,199] [INFO] [runner.py:568:main] cmd = mpirun -ppn 1 -genv PYTHONPATH /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference -genv OMP_NUM_THREADS 128 -genv MASTER_ADDR 10.242.51.166 -genv MASTER_PORT 29500 -genv WORLD_SIZE 2 -genv LOCAL_SIZE 1 -genv I_MPI_PIN 0 -hosts 10.242.51.166,10.242.51.116 -n 1 -env RANK 0 -env LOCAL_RANK 0 numactl -C 0-127 /root/miniforge3/envs/llm/bin/python -u run.py --benchmark -m ./saved_results/llama_8b_shard/ --dtype bfloat16 --greedy --input-tokens 1024 --num-iter 1 --batch-size 1 --max-new-tokens 56 --token-latency --autotp : -n 1 -env RANK 1 -env LOCAL_RANK 0 numactl -C 0-127 /root/miniforge3/envs/llm/bin/python -u run.py --benchmark -m ./saved_results/llama_8b_shard/ --dtype bfloat16 --greedy --input-tokens 1024 --num-iter 1 --batch-size 1 --max-new-tokens 56 --token-latency --autotp
[mpiexec@JF5300-B11A346T] Launch arguments: /usr/bin/ssh -x 10.242.51.166 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_bstrap_proxy --upstream-host JF5300-B11A346T --upstream-port 36813 --pgid 0 --launcher ssh --launcher-number 0 --base-path /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin/ --tree-width 16 --tree-level 1 --time-left -1 --launch-type 2 --debug --proxy-id 0 --node-id 0 --subtree-size 1 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_pmi_proxy --usize -1 --auto-cleanup 1 --abort-signal 9 
[mpiexec@JF5300-B11A346T] Launch arguments: /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_bstrap_proxy --upstream-host JF5300-B11A346T --upstream-port 36813 --pgid 0 --launcher ssh --launcher-number 0 --base-path /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin/ --tree-width 16 --tree-level 1 --time-left -1 --launch-type 2 --debug --proxy-id 1 --node-id 1 --subtree-size 1 --upstream-fd 11 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_pmi_proxy --usize -1 --auto-cleanup 1 --abort-signal 9 
LLM RUNTIME INFO: running model geneartion with deepspeed (autotp)...
LLM RUNTIME INFO: running model geneartion with deepspeed (autotp)...
/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 22:28:51,992] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
/root/miniforge3/envs/llm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:270: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 22:28:52,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Error.  nthreads cannot be larger than environment variable "NUMEXPR_MAX_THREADS" (64)Error.  nthreads cannot be larger than environment variable "NUMEXPR_MAX_THREADS" (64)[2024-11-03 22:28:53,128] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 22:28:53,128] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 22:28:53,128] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl
Using /root/.cache/torch_extensions/py310_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cpu/deepspeed_shm_comm/build.ninja...
Building extension module deepspeed_shm_comm...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module deepspeed_shm_comm...
[2024-11-03 22:28:53,285] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 22:28:53,285] [INFO] [comm.py:637:init_distributed] cdb=None
Using /root/.cache/torch_extensions/py310_cpu as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py310_cpu/deepspeed_shm_comm/build.ninja...
Building extension module deepspeed_shm_comm...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module deepspeed_shm_comm...
Time to load deepspeed_shm_comm op: 0.0548863410949707 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
*** Loading the model /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/saved_results/llama_8b_shard
[2024-11-03 22:28:53,648] [INFO] [utils.py:781:see_memory_usage] pre-from-pretrained
[2024-11-03 22:28:53,648] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-03 22:28:53,649] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.54 GB, percent = 1.7%
[2024-11-03 22:28:53,804] [INFO] [utils.py:781:see_memory_usage] post-from-pretrained
[2024-11-03 22:28:53,804] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-03 22:28:53,804] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.54 GB, percent = 1.7%
[2024-11-03 22:28:53,930] [INFO] [utils.py:781:see_memory_usage] post-init-ds-zero-init
[2024-11-03 22:28:53,931] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-03 22:28:53,931] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.54 GB, percent = 1.7%
[2024-11-03 22:28:54,055] [INFO] [utils.py:781:see_memory_usage] pre-ds-inference-init
[2024-11-03 22:28:54,055] [INFO] [utils.py:782:see_memory_usage] MA 0.64 GB         Max_MA 0.64 GB         CA 0.64 GB         Max_CA 1 GB 
[2024-11-03 22:28:54,055] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.54 GB, percent = 1.7%
My guessed rank = 0
My guessed rank = 1
[2024-11-03 22:28:55,014] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
Time to load deepspeed_shm_comm op: 0.060933589935302734 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
*** Loading the model /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/saved_results/llama_8b_shard
[2024-11-03 22:28:55,012] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
[2024-11-03 22:28:55,015] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead
[2024-11-03 22:28:55,016] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['self_attn.o_proj', 'mlp.down_proj'])]
Loading 34 checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]/root/miniforge3/envs/llm/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py:616: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint, map_location='cpu')
AutoTP:  [(<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>, ['mlp.down_proj', 'self_attn.o_proj'])]
Loading 34 checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]/root/miniforge3/envs/llm/lib/python3.10/site-packages/deepspeed/module_inject/replace_module.py:616: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(checkpoint, map_location='cpu')
Loading 34 checkpoint shards:   3%|▎         | 1/34 [00:00<00:11,  2.83it/s]Loading 34 checkpoint shards:   3%|▎         | 1/34 [00:00<00:11,  2.79it/s]Loading 34 checkpoint shards:   6%|▌         | 2/34 [00:00<00:09,  3.54it/s]Loading 34 checkpoint shards:   6%|▌         | 2/34 [00:00<00:09,  3.54it/s]Loading 34 checkpoint shards:   9%|▉         | 3/34 [00:00<00:07,  4.06it/s]Loading 34 checkpoint shards:   9%|▉         | 3/34 [00:00<00:07,  4.05it/s]Loading 34 checkpoint shards:  12%|█▏        | 4/34 [00:00<00:06,  4.47it/s]Loading 34 checkpoint shards:  12%|█▏        | 4/34 [00:00<00:06,  4.46it/s]Loading 34 checkpoint shards:  15%|█▍        | 5/34 [00:01<00:06,  4.75it/s]Loading 34 checkpoint shards:  15%|█▍        | 5/34 [00:01<00:06,  4.75it/s]Loading 34 checkpoint shards:  18%|█▊        | 6/34 [00:01<00:05,  4.94it/s]Loading 34 checkpoint shards:  18%|█▊        | 6/34 [00:01<00:05,  4.94it/s]Loading 34 checkpoint shards:  21%|██        | 7/34 [00:01<00:05,  5.08it/s]Loading 34 checkpoint shards:  21%|██        | 7/34 [00:01<00:05,  5.03it/s]Loading 34 checkpoint shards:  24%|██▎       | 8/34 [00:01<00:05,  5.16it/s]Loading 34 checkpoint shards:  24%|██▎       | 8/34 [00:01<00:05,  5.08it/s]Loading 34 checkpoint shards:  26%|██▋       | 9/34 [00:01<00:04,  5.24it/s]Loading 34 checkpoint shards:  26%|██▋       | 9/34 [00:01<00:04,  5.15it/s]Loading 34 checkpoint shards:  29%|██▉       | 10/34 [00:02<00:04,  5.35it/s]Loading 34 checkpoint shards:  29%|██▉       | 10/34 [00:02<00:04,  5.26it/s]Loading 34 checkpoint shards:  32%|███▏      | 11/34 [00:02<00:04,  5.43it/s]Loading 34 checkpoint shards:  32%|███▏      | 11/34 [00:02<00:04,  5.37it/s]Loading 34 checkpoint shards:  35%|███▌      | 12/34 [00:02<00:04,  5.50it/s]Loading 34 checkpoint shards:  35%|███▌      | 12/34 [00:02<00:04,  5.46it/s]Loading 34 checkpoint shards:  38%|███▊      | 13/34 [00:02<00:03,  5.53it/s]Loading 34 checkpoint shards:  38%|███▊      | 13/34 [00:02<00:03,  5.47it/s]Loading 34 checkpoint shards:  41%|████      | 14/34 [00:02<00:03,  5.57it/s]Loading 34 checkpoint shards:  41%|████      | 14/34 [00:02<00:03,  5.49it/s]Loading 34 checkpoint shards:  44%|████▍     | 15/34 [00:02<00:03,  5.51it/s]Loading 34 checkpoint shards:  44%|████▍     | 15/34 [00:03<00:03,  5.51it/s]Loading 34 checkpoint shards:  47%|████▋     | 16/34 [00:03<00:03,  5.54it/s]Loading 34 checkpoint shards:  47%|████▋     | 16/34 [00:03<00:03,  5.49it/s]Loading 34 checkpoint shards:  50%|█████     | 17/34 [00:03<00:03,  5.48it/s]Loading 34 checkpoint shards:  50%|█████     | 17/34 [00:03<00:03,  5.46it/s]Loading 34 checkpoint shards:  53%|█████▎    | 18/34 [00:03<00:02,  5.55it/s]Loading 34 checkpoint shards:  53%|█████▎    | 18/34 [00:03<00:02,  5.46it/s]Loading 34 checkpoint shards:  56%|█████▌    | 19/34 [00:03<00:02,  5.59it/s]Loading 34 checkpoint shards:  56%|█████▌    | 19/34 [00:03<00:02,  5.51it/s]Loading 34 checkpoint shards:  59%|█████▉    | 20/34 [00:03<00:02,  5.60it/s]Loading 34 checkpoint shards:  59%|█████▉    | 20/34 [00:03<00:02,  5.51it/s]Loading 34 checkpoint shards:  62%|██████▏   | 21/34 [00:04<00:02,  5.61it/s]Loading 34 checkpoint shards:  62%|██████▏   | 21/34 [00:04<00:02,  5.55it/s]Loading 34 checkpoint shards:  65%|██████▍   | 22/34 [00:04<00:02,  5.65it/s]Loading 34 checkpoint shards:  65%|██████▍   | 22/34 [00:04<00:02,  5.58it/s]Loading 34 checkpoint shards:  68%|██████▊   | 23/34 [00:04<00:01,  5.56it/s]Loading 34 checkpoint shards:  68%|██████▊   | 23/34 [00:04<00:02,  5.48it/s]Loading 34 checkpoint shards:  71%|███████   | 24/34 [00:04<00:01,  5.59it/s]Loading 34 checkpoint shards:  71%|███████   | 24/34 [00:04<00:01,  5.49it/s]Loading 34 checkpoint shards:  74%|███████▎  | 25/34 [00:04<00:01,  5.61it/s]Loading 34 checkpoint shards:  74%|███████▎  | 25/34 [00:04<00:01,  5.48it/s]Loading 34 checkpoint shards:  76%|███████▋  | 26/34 [00:04<00:01,  5.63it/s]Loading 34 checkpoint shards:  76%|███████▋  | 26/34 [00:04<00:01,  5.61it/s]Loading 34 checkpoint shards:  79%|███████▉  | 27/34 [00:05<00:01,  5.64it/s]Loading 34 checkpoint shards:  79%|███████▉  | 27/34 [00:05<00:01,  5.56it/s]Loading 34 checkpoint shards:  82%|████████▏ | 28/34 [00:05<00:01,  5.65it/s]Loading 34 checkpoint shards:  82%|████████▏ | 28/34 [00:05<00:01,  5.68it/s]Loading 34 checkpoint shards:  85%|████████▌ | 29/34 [00:05<00:00,  5.67it/s]Loading 34 checkpoint shards:  85%|████████▌ | 29/34 [00:05<00:00,  5.68it/s]Loading 34 checkpoint shards:  88%|████████▊ | 30/34 [00:05<00:00,  5.66it/s]Loading 34 checkpoint shards:  88%|████████▊ | 30/34 [00:05<00:00,  5.52it/s]Loading 34 checkpoint shards:  91%|█████████ | 31/34 [00:05<00:00,  5.60it/s]Loading 34 checkpoint shards:  91%|█████████ | 31/34 [00:05<00:00,  5.56it/s]Loading 34 checkpoint shards:  94%|█████████▍| 32/34 [00:06<00:00,  5.61it/s]Loading 34 checkpoint shards:  94%|█████████▍| 32/34 [00:06<00:00,  5.54it/s]Loading 34 checkpoint shards:  97%|█████████▋| 33/34 [00:06<00:00,  4.24it/s]Loading 34 checkpoint shards:  97%|█████████▋| 33/34 [00:06<00:00,  4.18it/s]Loading 34 checkpoint shards: 100%|██████████| 34/34 [00:06<00:00,  4.59it/s]Loading 34 checkpoint shards: 100%|██████████| 34/34 [00:06<00:00,  4.58it/s]Loading 34 checkpoint shards: 100%|██████████| 34/34 [00:06<00:00,  5.12it/s]
Loading 34 checkpoint shards: 100%|██████████| 34/34 [00:06<00:00,  5.07it/s]
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
[2024-11-03 22:29:01,869] [INFO] [utils.py:781:see_memory_usage] post-ds-inference-init
[2024-11-03 22:29:01,869] [INFO] [utils.py:782:see_memory_usage] MA 10.7 GB         Max_MA 10.7 GB         CA 10.7 GB         Max_CA 11 GB 
[2024-11-03 22:29:01,869] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.67 GB, percent = 3.7%
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
2024-11-03 22:29:06,156 - _logger.py - __main__ - WARNING - --token-latency requires using ipex (--ipex or --ipex-weight-only-quantization). Disabling --token-latency.
2024-11-03 22:29:06,160 - _logger.py - __main__ - WARNING - --token-latency requires using ipex (--ipex or --ipex-weight-only-quantization). Disabling --token-latency.
/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/distributed/run_generation_with_deepspeed.py:692: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
*** Starting to generate 56 tokens with bs=1
Generate args {'do_sample': False, 'num_beams': 1, 'max_new_tokens': 56, 'min_new_tokens': 56, 'streamer': None}
*** Prompt size:  938
[2024-11-03 22:29:06,345] [INFO] [utils.py:781:see_memory_usage] end-of-run
[2024-11-03 22:29:06,346] [INFO] [utils.py:782:see_memory_usage] MA 10.71 GB         Max_MA 10.71 GB         CA 10.71 GB         Max_CA 11 GB 
[2024-11-03 22:29:06,346] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.71 GB, percent = 3.7%
/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/distributed/run_generation_with_deepspeed.py:692: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
*** Running benchmark
("It is done, and submitted. You can play 'Survival of the Tastiest' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'd like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like 'Evolution' in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it's not evolution anymore - it's the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that's not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn't want to create an 'intelligent design' simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I'd say the only real solution was through the use of artificial selection, somehow. So far, I have not seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn't think of compelling (read: serious) mechanics for that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey (who also created the 'Mechanical Underdogs' signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your 'base'. There are 5 other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You do so by 'ordering' them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying 'costs', which are debited from your credits (you start with a number of credits). Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps). Your pasta doesn't like other people's pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill. Once a pasta is in the vicinity of a plate, it can try to conquer it. If it succeeds, it becomes the new pasta of that plate. If it fails, it dies. If it dies, it becomes a new pasta, and the cycle repeats. The game ends when you have conquered all the plates, or when you", 56)
Iteration: 0, Time: 11.196406 sec
inftime=-0.0  BS=1 token=56

gflops=0.0


 ---------- Summary: ----------
Inference latency: -0.00 ms.
*** Starting to generate 56 tokens with bs=1
Generate args {'do_sample': False, 'num_beams': 1, 'max_new_tokens': 56, 'min_new_tokens': 56, 'streamer': None}
*** Prompt size:  938
*** Running benchmark
("It is done, and submitted. You can play 'Survival of the Tastiest' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'd like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like 'Evolution' in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it's not evolution anymore - it's the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that's not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn't want to create an 'intelligent design' simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I'd say the only real solution was through the use of artificial selection, somehow. So far, I have not seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn't think of compelling (read: serious) mechanics for that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey (who also created the 'Mechanical Underdogs' signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your 'base'. There are 5 other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You do so by 'ordering' them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying 'costs', which are debited from your credits (you start with a number of credits). Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps). Your pasta doesn't like other people's pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill. Once a pasta is in the vicinity of a plate, it can try to conquer it. If it succeeds, it becomes the new pasta of that plate. If it fails, it dies. If it dies, it becomes a new pasta, and the cycle repeats. The game ends when you have conquered all the plates, or when you", 56)
Iteration: 0, Time: 11.252740 sec
inftime=-0.0  BS=1 token=56

gflops=0.0


 ---------- Summary: ----------
Inference latency: -0.00 ms.
LLM RUNTIME INFO: Finished successfully.
LLM RUNTIME INFO: Finished successfully.

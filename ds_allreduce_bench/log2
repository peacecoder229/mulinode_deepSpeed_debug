Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 21:22:01,469] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2024-11-03 21:22:03,291] [INFO] [runner.py:463:main] Using IP address of 10.242.51.166 for node 10.242.51.166
['mpirun', '-ppn', '2', '-genv', 'PYTHONPATH', '/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/ds_allreduce_bench', '-genv', 'OMP_NUM_THREADS', '64', '-genv', 'MASTER_ADDR', '10.242.51.166', '-genv', 'MASTER_PORT', '29500', '-genv', 'WORLD_SIZE', '4', '-genv', 'LOCAL_SIZE', '2', '-genv', 'I_MPI_PIN', '0', '-hosts', '10.242.51.166,10.242.51.116', '-n', '1', '-env', 'RANK', '0', '-env', 'LOCAL_RANK', '0', 'numactl', '-C', '0-63', '/root/miniforge3/envs/llm/bin/python', '-u', './ds_comm_bench_compare_matmul_vs_allreduce.py', '--dtype', 'bf16', '--count', '5', '--warmup', '2', '--elements', '67108864', '--computeSz', '1024', ':', '-n', '1', '-env', 'RANK', '1', '-env', 'LOCAL_RANK', '1', 'numactl', '-C', '64-127', '/root/miniforge3/envs/llm/bin/python', '-u', './ds_comm_bench_compare_matmul_vs_allreduce.py', '--dtype', 'bf16', '--count', '5', '--warmup', '2', '--elements', '67108864', '--computeSz', '1024', ':', '-n', '1', '-env', 'RANK', '2', '-env', 'LOCAL_RANK', '0', 'numactl', '-C', '0-63', '/root/miniforge3/envs/llm/bin/python', '-u', './ds_comm_bench_compare_matmul_vs_allreduce.py', '--dtype', 'bf16', '--count', '5', '--warmup', '2', '--elements', '67108864', '--computeSz', '1024', ':', '-n', '1', '-env', 'RANK', '3', '-env', 'LOCAL_RANK', '1', 'numactl', '-C', '64-127', '/root/miniforge3/envs/llm/bin/python', '-u', './ds_comm_bench_compare_matmul_vs_allreduce.py', '--dtype', 'bf16', '--count', '5', '--warmup', '2', '--elements', '67108864', '--computeSz', '1024']
[2024-11-03 21:22:03,367] [INFO] [runner.py:568:main] cmd = mpirun -ppn 2 -genv PYTHONPATH /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/inference/ds_allreduce_bench -genv OMP_NUM_THREADS 64 -genv MASTER_ADDR 10.242.51.166 -genv MASTER_PORT 29500 -genv WORLD_SIZE 4 -genv LOCAL_SIZE 2 -genv I_MPI_PIN 0 -hosts 10.242.51.166,10.242.51.116 -n 1 -env RANK 0 -env LOCAL_RANK 0 numactl -C 0-63 /root/miniforge3/envs/llm/bin/python -u ./ds_comm_bench_compare_matmul_vs_allreduce.py --dtype bf16 --count 5 --warmup 2 --elements 67108864 --computeSz 1024 : -n 1 -env RANK 1 -env LOCAL_RANK 1 numactl -C 64-127 /root/miniforge3/envs/llm/bin/python -u ./ds_comm_bench_compare_matmul_vs_allreduce.py --dtype bf16 --count 5 --warmup 2 --elements 67108864 --computeSz 1024 : -n 1 -env RANK 2 -env LOCAL_RANK 0 numactl -C 0-63 /root/miniforge3/envs/llm/bin/python -u ./ds_comm_bench_compare_matmul_vs_allreduce.py --dtype bf16 --count 5 --warmup 2 --elements 67108864 --computeSz 1024 : -n 1 -env RANK 3 -env LOCAL_RANK 1 numactl -C 64-127 /root/miniforge3/envs/llm/bin/python -u ./ds_comm_bench_compare_matmul_vs_allreduce.py --dtype bf16 --count 5 --warmup 2 --elements 67108864 --computeSz 1024
[mpiexec@JF5300-B11A346T] Launch arguments: /usr/bin/ssh -x 10.242.51.166 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_bstrap_proxy --upstream-host JF5300-B11A346T --upstream-port 34845 --pgid 0 --launcher ssh --launcher-number 0 --base-path /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin/ --tree-width 16 --tree-level 1 --time-left -1 --launch-type 2 --debug --proxy-id 0 --node-id 0 --subtree-size 1 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_pmi_proxy --usize -1 --auto-cleanup 1 --abort-signal 9 
[mpiexec@JF5300-B11A346T] Launch arguments: /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_bstrap_proxy --upstream-host JF5300-B11A346T --upstream-port 34845 --pgid 0 --launcher ssh --launcher-number 0 --base-path /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin/ --tree-width 16 --tree-level 1 --time-left -1 --launch-type 2 --debug --proxy-id 1 --node-id 1 --subtree-size 1 --upstream-fd 11 /home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/bin//hydra_pmi_proxy --usize -1 --auto-cleanup 1 --abort-signal 9 
IPL WARN> ipl_create_domains: Not enough cpus for the specified I_MPI_PIN_ORDER, switch to I_MPI_PIN_ORDER=compact
My guessed rank = 1
My guessed rank = 0
My guessed rank = 3
My guessed rank = 2
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 21:22:06,122] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 21:22:06,128] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 21:22:06,187] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /root/miniforge3/envs/llm/lib/python3.10/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory
[2024-11-03 21:22:06,210] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2024-11-03 21:22:07,256] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 21:22:07,256] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 21:22:07,257] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 21:22:07,258] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 21:22:07,258] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend ccl
ninja: no work to do.
Time to load deepspeed_shm_comm op: 0.05478501319885254 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
Time to load deepspeed_shm_comm op: 0.10068440437316895 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
[2024-11-03 21:22:07,410] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 21:22:07,410] [INFO] [comm.py:637:init_distributed] cdb=None
ninja: no work to do.
Time to load deepspeed_shm_comm op: 0.06118512153625488 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
[2024-11-03 21:22:07,476] [INFO] [comm.py:161:init_deepspeed_backend] Initialize ccl backend
[2024-11-03 21:22:07,476] [INFO] [comm.py:637:init_distributed] cdb=None
ninja: no work to do.
Time to load deepspeed_shm_comm op: 0.06100893020629883 seconds
DeepSpeed deepspeed.ops.comm.deepspeed_shm_comm_op built successfully
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=init pmi_version=1 pmi_subversion=1
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=response_to_init pmi_version=1 pmi_subversion=1 rc=0
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get_maxes
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=maxes kvsname_max=256 keylen_max=64 vallen_max=4096
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get_appnum
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=appnum appnum=1
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get_my_kvsname
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=my_kvsname kvsname=kvs_218791_0
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get kvsname=kvs_218791_0 key=PMI_process_mapping
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=get_result rc=0 msg=success value=(vector,(0,2,2))
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=barrier_in
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=init pmi_version=1 pmi_subversion=1
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=response_to_init pmi_version=1 pmi_subversion=1 rc=0
MPI startup(): Run 'pmi_process_mapping' nodemap algorithm
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=get_maxes
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=maxes kvsname_max=256 keylen_max=64 vallen_max=4096
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=get_appnum
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=appnum appnum=0
[0] MPI startup(): Intel(R) MPI Library, Version 2021.12  Build 20240202 (id: c8dd3f5)
[0] MPI startup(): Copyright (C) 2003-2024 Intel Corporation.  All rights reserved.
[0] MPI startup(): library kind: release
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=get_my_kvsname
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=my_kvsname kvsname=kvs_218791_0
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=get kvsname=kvs_218791_0 key=PMI_process_mapping
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=get_result rc=0 msg=success value=(vector,(0,2,2))
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=put kvsname=kvs_218791_0 key=-bcast-1-0 value=2F6465762F73686D2F496E74656C5F4D50495F6A4171647261
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=put_result rc=0 msg=success
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=barrier_in
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=init pmi_version=1 pmi_subversion=1
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=response_to_init pmi_version=1 pmi_subversion=1 rc=0
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get_maxes
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=maxes kvsname_max=256 keylen_max=64 vallen_max=4096
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get_appnum
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=appnum appnum=3
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get_my_kvsname
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=my_kvsname kvsname=kvs_218791_0
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=init pmi_version=1 pmi_subversion=1
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=response_to_init pmi_version=1 pmi_subversion=1 rc=0
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get kvsname=kvs_218791_0 key=PMI_process_mapping
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=get_result rc=0 msg=success value=(vector,(0,2,2))
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=get_maxes
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=maxes kvsname_max=256 keylen_max=64 vallen_max=4096
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=get_appnum
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=appnum appnum=2
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=barrier_in
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=get_my_kvsname
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=my_kvsname kvsname=kvs_218791_0
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=get kvsname=kvs_218791_0 key=PMI_process_mapping
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=get_result rc=0 msg=success value=(vector,(0,2,2))
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=put kvsname=kvs_218791_0 key=-bcast-1-2 value=2F6465762F73686D2F496E74656C5F4D50495F673043645A6F
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=put_result rc=0 msg=success
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=barrier_in
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=barrier_out
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=barrier_out
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get kvsname=kvs_218791_0 key=-bcast-1-2
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=get_result rc=0 msg=success value=2F6465762F73686D2F496E74656C5F4D50495F673043645A6F
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=barrier_out
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=barrier_out
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get kvsname=kvs_218791_0 key=-bcast-1-0
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=get_result rc=0 msg=success value=2F6465762F73686D2F496E74656C5F4D50495F6A4171647261
[0] MPI startup(): libfabric loaded: libfabric.so.1 
[0] MPI startup(): libfabric version: 1.18.1-impi
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=barrier_in
[0] MPI startup(): max_ch4_vnis: 1, max_reg_eps 64, enable_sep 0, enable_shared_ctxs 0, do_av_insert 0
[0] MPI startup(): max number of MPI_Request per vci: 67108864 (pools: 1)
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=barrier_in
[0] MPI startup(): libfabric provider: tcp
[0] MPI startup(): detected tcp provider, set device name to "tcp"
[0] MPI startup(): addrnamelen: 16
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=put kvsname=kvs_218791_0 key=bc-0 value=mpi#02008FF3C0A88C7F0000000000000000$
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=put_result rc=0 msg=success
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=barrier_in
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=put kvsname=kvs_218791_0 key=bc-2 value=mpi#02008613C0A88C7A0000000000000000$
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=put_result rc=0 msg=success
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=barrier_in
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=barrier_out
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=barrier_out
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 8: cmd=get kvsname=kvs_218791_0 key=bc-0
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=get_result rc=0 msg=success value=mpi#02008FF3C0A88C7F0000000000000000$
[proxy:0:1@JF5300-B11A346T] pmi cmd from fd 9: cmd=get kvsname=kvs_218791_0 key=bc-2
[proxy:0:1@JF5300-B11A346T] PMI response: cmd=get_result rc=0 msg=success value=mpi#02008613C0A88C7A0000000000000000$
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=barrier_out
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=barrier_out
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 4: cmd=get kvsname=kvs_218791_0 key=bc-0
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=get_result rc=0 msg=success value=mpi#02008FF3C0A88C7F0000000000000000$
[proxy:0:0@JF5300-B11A319T] pmi cmd from fd 5: cmd=get kvsname=kvs_218791_0 key=bc-2
[proxy:0:0@JF5300-B11A319T] PMI response: cmd=get_result rc=0 msg=success value=mpi#02008613C0A88C7A0000000000000000$
[2] MPI startup(): shm segment size (1211 MB per rank) * (2 local ranks) = 2423 MB total
[0] MPI startup(): shm segment size (1211 MB per rank) * (2 local ranks) = 2423 MB total
[2] MPI startup(): NUMA nodes mask 0x5
[0] MPI startup(): NUMA nodes mask 0x5
[3] MPI startup(): selected platform: emr
[2] MPI startup(): selected platform: emr
[1] MPI startup(): selected platform: emr
[0] MPI startup(): selected platform: emr
[0] MPI startup(): File "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc/tuning_spr_shm-ofi_tcp_100_x1.dat" not found
[0] MPI startup(): File "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc/tuning_spr_shm-ofi_tcp_100.dat" not found
[0] MPI startup(): File "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc/tuning_spr_shm-ofi_tcp.dat" not found
[0] MPI startup(): Load tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc/tuning_spr_shm-ofi.dat"
[0] MPI startup(): File "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc/tuning_spr_shm-ofi.dat" not found
[0] MPI startup(): Looking for tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc//tuning_clx-ap_shm-ofi_tcp.dat"
[0] MPI startup(): Looking for tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc//tuning_skx_shm-ofi_tcp.dat"
[0] MPI startup(): Looking for tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc//tuning_generic_shm-ofi_tcp.dat"
[0] MPI startup(): Looking for tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc//tuning_clx-ap_shm-ofi.dat"
[0] MPI startup(): Load tuning file: "/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi/etc//tuning_clx-ap_shm-ofi.dat"
[2] MPI startup(): Number of NICs:  1 
[2] MPI startup(): ===== NIC pinning on JF5300-B11A346T =====
[2] MPI startup(): Rank    Pin nic   Nic Id
[2] MPI startup(): 2       ens11np0  0
[2] MPI startup(): 3       ens11np0  0
[0] MPI startup(): threading: mode: direct
[0] MPI startup(): threading: vcis: 1
[0] MPI startup(): threading: app_threads: 1
[0] MPI startup(): threading: runtime: generic
[0] MPI startup(): threading: progress_threads: 1
[0] MPI startup(): threading: async_progress: 0
[0] MPI startup(): threading: lock_level: global
[0] MPI startup(): threading: num_pools: 1
[0] MPI startup(): threading: enable_sep: 0
[0] MPI startup(): threading: direct_recv: 1
[0] MPI startup(): threading: zero_op_flags: 1
[0] MPI startup(): threading: num_am_buffers: 1
[0] MPI startup(): tag bits available: 19 (TAG_UB value: 524287) 
[0] MPI startup(): source bits available: 20 (Maximal number of rank: 1048575) 
[0] MPI startup(): Number of NICs:  1 
[0] MPI startup(): ===== NIC pinning on JF5300-B11A319T =====
[0] MPI startup(): Rank    Pin nic  Nic Id
[0] MPI startup(): 0       ens1np0  0
[0] MPI startup(): 1       ens1np0  0
[0] MPI startup(): THREAD_SPLIT mode is switched on, 1 endpoints in use
[0] MPI startup(): ===== CPU pinning =====
[0] MPI startup(): Rank    Pid      Node name        Pin cpu
[0] MPI startup(): 0       195014   JF5300-B11A319T  {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,
                                       30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56
                                       ,57,58,59,60,61,62,63}
[0] MPI startup(): 1       195015   JF5300-B11A319T  {64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
                                       ,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,
                                       113,114,115,116,117,118,119,120,121,122,123,124,125,126,127}
[0] MPI startup(): 2       218796   JF5300-B11A346T  {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,
                                       30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56
                                       ,57,58,59,60,61,62,63}
[0] MPI startup(): 3       218797   JF5300-B11A346T  {64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90
                                       ,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,
                                       113,114,115,116,117,118,119,120,121,122,123,124,125,126,127}
[0] MPI startup(): I_MPI_ROOT=/home/rdas/LLM/intel-extension-for-pytorch/examples/cpu/llm/oneCCL_release/opt/mpi
[0] MPI startup(): I_MPI_MPIRUN=mpirun
[0] MPI startup(): I_MPI_BIND_WIN_ALLOCATE=localalloc
[0] MPI startup(): I_MPI_HYDRA_TOPOLIB=hwloc
[0] MPI startup(): I_MPI_RETURN_WIN_MEM_NUMA=0
[0] MPI startup(): I_MPI_PIN=0
[0] MPI startup(): I_MPI_INTERNAL_MEM_POLICY=default
[0] MPI startup(): I_MPI_OFI_ISEND_INJECT_THRESHOLD=0
[0] MPI startup(): I_MPI_MULTIRAIL=1
[0] MPI startup(): I_MPI_THREAD_SPLIT=1
[0] MPI startup(): I_MPI_THREAD_RUNTIME=generic
[0] MPI startup(): I_MPI_THREAD_ID_KEY=vci
[0] MPI startup(): I_MPI_THREAD_MAX=1
[0] MPI startup(): I_MPI_THREAD_LOCK_LEVEL=global
[0] MPI startup(): I_MPI_DEBUG=120
[2] allocate handle (kind=1, size=744, direct_size=8, indirect_size=1) ptr=0x55e9ef8a6000
[2] allocate handle (kind=2, size=40, direct_size=8, indirect_size=1) ptr=0x55e9dad8e000
[3] allocate handle (kind=2, size=40, direct_size=8, indirect_size=1) ptr=0x55beb7b2c000
[0] allocate handle (kind=1, size=744, direct_size=8, indirect_size=1) ptr=0x55d23bf52000
[0] allocate handle (kind=2, size=40, direct_size=8, indirect_size=1) ptr=0x55d227464000
[1] allocate handle (kind=2, size=40, direct_size=8, indirect_size=1) ptr=0x563066e1e000
[3] allocate handle (kind=1, size=744, direct_size=8, indirect_size=1) ptr=0x55becc65e000
[2] allocate handle (kind=7, size=32, direct_size=8, indirect_size=1) ptr=0x55e9efbce000
[1] allocate handle (kind=1, size=744, direct_size=8, indirect_size=1) ptr=0x56307c0e8000
[3] allocate handle (kind=7, size=32, direct_size=8, indirect_size=1) ptr=0x55becc966000
[0] allocate handle (kind=7, size=32, direct_size=8, indirect_size=1) ptr=0x55d23c27c000
[1] allocate handle (kind=7, size=32, direct_size=8, indirect_size=1) ptr=0x56307c3f0000
[0] max rel diff with ref 0.010302109643816948
tensor([1.8672, 1.5312, 1.4141,  ..., 2.0938, 1.7812, 2.8594],
       dtype=torch.bfloat16)
iteration 0 of 5iteration 1 of 5iteration 2 of 5iteration 3 of 5iteration 4 of 5iteration 5 of 5iteration 6 of 5num_elements = 67108864, dtype = torch.bfloat16, allreduce use shm
Average matrix multiplication duration = 0.5600 ms
Average allreduce duration = 161.3743 ms
